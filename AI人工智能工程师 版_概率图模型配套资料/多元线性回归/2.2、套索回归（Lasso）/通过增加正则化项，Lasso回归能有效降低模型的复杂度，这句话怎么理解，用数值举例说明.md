通过增加正则化项，Lasso回归能有效降低模型的复杂度，主要是通过缩小系数的大小，甚至将一些系数压缩到零，从而简化模型，减少不必要的特征对模型的影响。

让我们用一个简单的例子来说明这一点。

### 假设的线性回归模型

假设我们有一个线性回归模型，包含 4 个特征 \( X_1, X_2, X_3, X_4 \)：

\[
\hat{y} = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4
\]

在普通线性回归中，我们的目标是最小化误差（残差平方和），可能会得到这样的系数：
\[
\beta_1 = 3.5, \beta_2 = 2.7, \beta_3 = 1.2, \beta_4 = 4.1
\]

这里所有特征的系数都不为零，表明每个特征都被用来预测 \( y \)。

### Lasso回归的引入

现在我们引入Lasso回归，通过增加一个正则化项（例如 \( \lambda = 1 \)），新的损失函数如下：

\[
\text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{4} |\beta_j|
\]

正则化项的作用是对系数施加惩罚，促使它们变得更小，甚至为零。假设我们得到了以下结果：

\[
\beta_1 = 3.0, \beta_2 = 2.3, \beta_3 = 0, \beta_4 = 3.6
\]

### 通过数值解释复杂度降低

1. **系数的缩小**：相比于没有正则化的情况（普通线性回归），Lasso回归中的系数变小了。比如，\( \beta_1 \) 从 3.5 缩小到 3.0，\( \beta_2 \) 从 2.7 缩小到 2.3，\( \beta_4 \) 从 4.1 缩小到 3.6。这意味着模型对每个特征的依赖减弱，降低了模型的复杂度。

2. **系数变为零**：最显著的变化是 \( \beta_3 \) 被压缩到 0，这表示 Lasso 回归认为特征 \( X_3 \) 对预测目标 \( y \) 的影响不大，因此将其从模型中移除。这进一步减少了模型的复杂度，因为现在模型只依赖 3 个特征而不是 4 个。

### 模型复杂度的变化

- **普通线性回归**：所有特征的系数都不为零，模型依赖所有特征来进行预测，模型的复杂度较高。
- **Lasso回归**：通过引入正则化，Lasso 回归压缩了一些系数，甚至将某些不重要的特征系数缩减为零。这不仅简化了模型结构，还降低了模型的复杂度。

### 总结

通过增加正则化项，Lasso 回归不仅可以缩小回归系数，还能够将不重要的系数设为零，从而减少模型依赖的特征数量，简化模型结构，降低模型的复杂度，最终提高模型的泛化能力。
