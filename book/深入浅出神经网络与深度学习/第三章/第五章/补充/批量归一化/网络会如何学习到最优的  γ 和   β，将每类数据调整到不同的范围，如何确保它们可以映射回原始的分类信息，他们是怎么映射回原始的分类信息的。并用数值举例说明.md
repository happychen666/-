网络是通过反向传播和梯度下降的方式，来学习最优的 \(\gamma\) 和 \(\beta\)，将每类数据调整到不同的范围，并确保它们可以映射回原始的分类信息。下面，我将通过具体的数值举例说明这个过程。

### 1. 批量归一化的前向传播过程

假设我们有两个类别的数据：

- 类别 1 的输入数据：\( h_1 = [50, 52, 54, 56] \)
- 类别 2 的输入数据：\( h_2 = [2, 4, 6, 8] \)

**目标**：我们希望通过批量归一化处理后，网络能够区分这两个类别的数据。批量归一化会对数据进行标准化处理，然后通过缩放 (\(\gamma\)) 和偏移 (\(\beta\)) 来恢复数据的分布。

#### 1.1 数据归一化

对于每一类数据，批量归一化会将其标准化为均值为0，方差为1的标准正态分布：

1. **类别 1 的标准化：**
   - 均值：
     \[
     \mathbb{E}[h_1] = \frac{50 + 52 + 54 + 56}{4} = 53
     \]
   - 方差：
     \[
     \text{Var}(h_1) = \frac{(50 - 53)^2 + (52 - 53)^2 + (54 - 53)^2 + (56 - 53)^2}{4} = \frac{9 + 1 + 1 + 9}{4} = 5
     \]
   - 标准化后的数据：
     \[
     \hat{h}_1 = \frac{h_1 - 53}{\sqrt{5}} \approx [-1.34, -0.45, 0.45, 1.34]
     \]

2. **类别 2 的标准化：**
   - 均值：
     \[
     \mathbb{E}[h_2] = \frac{2 + 4 + 6 + 8}{4} = 5
     \]
   - 方差：
     \[
     \text{Var}(h_2) = \frac{(2 - 5)^2 + (4 - 5)^2 + (6 - 5)^2 + (8 - 5)^2}{4} = 5
     \]
   - 标准化后的数据：
     \[
     \hat{h}_2 = \frac{h_2 - 5}{\sqrt{5}} \approx [-1.34, -0.45, 0.45, 1.34]
     \]

#### 1.2 缩放和偏移

接下来，我们引入可学习的参数 \(\gamma\) 和 \(\beta\)，用来将标准化后的数据调整回适合分类的分布。

假设网络当前初始化了以下参数：

- 类别 1 的 \(\gamma_1 = 5\)，\(\beta_1 = 10\)
- 类别 2 的 \(\gamma_2 = 2\)，\(\beta_2 = 3\)

1. **类别 1 的缩放和平移：**
   \[
   h_{BN_1} = \gamma_1 \cdot \hat{h}_1 + \beta_1 = 5 \cdot [-1.34, -0.45, 0.45, 1.34] + 10 \approx [3.3, 7.75, 12.25, 16.7]
   \]

2. **类别 2 的缩放和平移：**
   \[
   h_{BN_2} = \gamma_2 \cdot \hat{h}_2 + \beta_2 = 2 \cdot [-1.34, -0.45, 0.45, 1.34] + 3 \approx [0.32, 2.1, 3.9, 5.68]
   \]

这样，通过 \(\gamma\) 和 \(\beta\)，归一化后的数据重新调整回了不同的分布范围，使得类别 1 和类别 2 的数据在特征空间中仍然可以区分。

### 2. 反向传播中的 \(\gamma\) 和 \(\beta\) 学习

神经网络通过**反向传播**，根据损失函数的梯度调整 \(\gamma\) 和 \(\beta\)，从而逐步优化模型的分类能力。

#### 2.1 损失函数

假设我们使用交叉熵损失函数（或其他常见的损失函数）来衡量网络预测值与真实标签之间的差距。

- 类别 1 的标签是 1
- 类别 2 的标签是 0

在每次前向传播过程中，网络根据当前的 \(\gamma\) 和 \(\beta\) 参数计算出预测的分类结果。反向传播会通过计算损失函数的梯度来调整这些参数，使得每次调整后网络的输出更加接近正确的分类。

#### 2.2 反向传播计算

反向传播过程中，神经网络会计算损失函数对 \(\gamma\) 和 \(\beta\) 的梯度。假设当前损失为 \(L\)，我们需要计算：

- \(\frac{\partial L}{\partial \gamma}\)
- \(\frac{\partial L}{\partial \beta}\)

梯度下降法会根据这些梯度调整 \(\gamma\) 和 \(\beta\)：

\[
\gamma_{\text{new}} = \gamma_{\text{old}} - \eta \frac{\partial L}{\partial \gamma}
\]
\[
\beta_{\text{new}} = \beta_{\text{old}} - \eta \frac{\partial L}{\partial \beta}
\]

其中 \(\eta\) 是学习率。经过多轮训练后，\(\gamma\) 和 \(\beta\) 会被调整到一个最优值，使得网络能够更好地分类不同类别的数据。

### 3. 数值示例：如何映射回原始分类

为了理解如何映射回原始分类，假设网络的初始 \(\gamma\) 和 \(\beta\) 值不理想，导致输出的分类不准确。随着训练的进行，\(\gamma\) 和 \(\beta\) 会逐渐调整到使得每个类别的数据在特征空间中分布不同。

1. **初始情况：**
   - 类别 1 的输出：\( h_{BN_1} = [7, 7, 7, 7] \) （过于集中的值，难以区分）
   - 类别 2 的输出：\( h_{BN_2} = [7, 7, 7, 7] \) （与类别 1 的数据重叠）

此时，网络的损失较大，因为两类数据在特征空间中没有区分度。

2. **经过训练：**
   经过反向传播后，\(\gamma\) 和 \(\beta\) 被调整：
   - 类别 1 的输出：\( h_{BN_1} = [3.3, 7.75, 12.25, 16.7] \)
   - 类别 2 的输出：\( h_{BN_2} = [0.32, 2.1, 3.9, 5.68] \)

现在，类别 1 和类别 2 的数据在特征空间中已经有了不同的分布，网络可以通过这些不同的特征进行分类。

### 4. 映射回原始分类信息

最终，经过训练后，\(\gamma\) 和 \(\beta\) 能够让批量归一化后的数据重新获得区分度，从而映射回原始的分类信息。具体来说：

- **原始分布的恢复**：\(\gamma\) 控制特征的放大或缩小，\(\beta\) 控制特征的整体偏移。它们共同作用，将标准正态分布的归一化数据映射回与原始数据类似的分布。
- **确保分类信息不丢失**：通过调整 \(\gamma\) 和 \(\beta\)，网络能够恢复不同类别的区分度，确保模型在批量归一化后仍然可以正确地进行分类。

### 总结

- \(\gamma\) 和 \(\beta\) 的可学习参数通过反向传播和梯度下降进行更新，调整归一化后的数据，使其重新映射回适合分类的特征空间。
- \(\gamma\) 控制缩放，\(\beta\) 控制偏移，确保不同类别的数据在特征空间中分布不同。
- 网络通过训练自动学习最优的 \(\gamma\) 和 \(\beta\)，从而确保批量归一化不会丢失模型的分类能力。
