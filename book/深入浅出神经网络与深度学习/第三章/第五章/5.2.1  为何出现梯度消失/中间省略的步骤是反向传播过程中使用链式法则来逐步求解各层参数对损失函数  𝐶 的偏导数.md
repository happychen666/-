公式 \(\Delta C \approx \sigma'(z_1) w_2 \sigma'(z_2) \cdots \sigma'(z_4) \frac{\partial C}{\partial a_4} \Delta b_1\) 中间省略的步骤是反向传播过程中使用链式法则来逐步求解各层参数对损失函数 \(C\) 的偏导数。

为了详细推导这个公式，我们需要回顾反向传播（Backpropagation）算法，它通过链式法则逐层传播误差，并计算每一层参数（如权重和偏置）对损失函数的影响。

### 1. 损失函数的偏导数

假设神经网络有多层，每一层的输入和输出都经过一个激活函数 \(\sigma\)。定义：

- \(z_j\) 为第 \(j\) 层的线性组合结果（即 \(z_j = w_j \cdot a_{j-1} + b_j\)）。
- \(a_j = \sigma(z_j)\) 是激活值。
- \(C\) 是损失函数。
  
我们要推导的是：偏置 \(b_1\) 的微小变化如何影响损失函数 \(C\) 的变化。

### 2. 链式法则的应用

首先，损失函数 \(C\) 最终依赖于输出层的激活值 \(a_L\)（假设 \(L\) 是网络的层数）。因此，可以通过链式法则，将 \(C\) 对偏置 \(b_1\) 的影响逐层传播，表达为：
\[
\frac{\partial C}{\partial b_1} = \frac{\partial C}{\partial a_4} \cdot \frac{\partial a_4}{\partial z_4} \cdot \frac{\partial z_4}{\partial a_3} \cdot \frac{\partial a_3}{\partial z_3} \cdot \frac{\partial z_3}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1}
\]

我们逐个求出这些偏导数。

### 3. 求各层的偏导数

从输出层开始反向传播：

- \(\frac{\partial C}{\partial a_4}\)：这是损失函数 \(C\) 对输出层激活值 \(a_4\) 的偏导数，假设我们已经可以直接得到。
- \(\frac{\partial a_4}{\partial z_4} = \sigma'(z_4)\)：这是激活函数 \(\sigma\) 对 \(z_4\) 的导数。
- \(\frac{\partial z_4}{\partial a_3} = w_4\)：因为 \(z_4 = w_4 \cdot a_3 + b_4\)。
- \(\frac{\partial a_3}{\partial z_3} = \sigma'(z_3)\)：同样，这是激活函数对 \(z_3\) 的导数。
- \(\frac{\partial z_3}{\partial a_2} = w_3\)：类似地，\(z_3 = w_3 \cdot a_2 + b_3\)。
- \(\frac{\partial a_2}{\partial z_2} = \sigma'(z_2)\)。
- \(\frac{\partial z_2}{\partial a_1} = w_2\)。
- \(\frac{\partial a_1}{\partial z_1} = \sigma'(z_1)\)。
- \(\frac{\partial z_1}{\partial b_1} = 1\)，因为 \(z_1 = w_1 \cdot a_0 + b_1\)，偏置 \(b_1\) 的系数是1。

### 4. 组合所有偏导数

现在我们将所有这些偏导数相乘，得到：
\[
\frac{\partial C}{\partial b_1} = \sigma'(z_1) w_2 \sigma'(z_2) w_3 \sigma'(z_3) w_4 \sigma'(z_4) \frac{\partial C}{\partial a_4}
\]

如果考虑到 \(\Delta b_1\) 是一个微小的变化量，那么损失函数的变化可以近似表示为：
\[
\Delta C \approx \frac{\partial C}{\partial b_1} \Delta b_1
\]
将前面的结果代入：
\[
\Delta C \approx \sigma'(z_1) w_2 \sigma'(z_2) w_3 \sigma'(z_3) w_4 \sigma'(z_4) \frac{\partial C}{\partial a_4} \Delta b_1
\]

### 5. 公式的简化

在公式 \(\Delta C \approx \sigma'(z_1) w_2 \sigma'(z_2) \cdots \sigma'(z_4) \frac{\partial C}{\partial a_4} \Delta b_1\) 中，权重 \(w_3, w_4\) 被省略了，但实际上这些权重仍然在链式法则的传递中影响到损失函数。

因此，完整的推导过程没有特别大的省略，主要是链式法则的逐层求导过程，并将每一层的偏导数依次相乘，直到将 \(b_1\) 对损失函数 \(C\) 的影响计算完毕。

### 6. 结论

公式 \(\Delta C \approx \sigma'(z_1) w_2 \sigma'(z_2) \cdots \sigma'(z_4) \frac{\partial C}{\partial a_4} \Delta b_1\) 是通过链式法则推导出来的。在神经网络的反向传播过程中，误差逐层传播，通过激活函数的导数和权重传递到最终的偏置 \(b_1\)，从而影响损失函数的变化。这种方法是反向传播算法的核心，用来计算网络中每个参数的梯度。
