为了推导公式 \(\Delta z_2 \approx \sigma'(z_1) w_2 \Delta b_1\)，我们需要从神经网络的基本构成出发，理解如何通过链式法则计算神经网络中偏置对输出的影响。

### 1. 神经网络层间的传播关系

我们从神经网络的一般形式开始。假设第一个隐藏层的激活值 \(a_1\) 是通过输入 \(x\) 和偏置 \(b_1\) 经过激活函数的作用得到的。它的表达式为：
\[
a_1 = \sigma(z_1)
\]
其中：
\[
z_1 = w_1 \cdot x + b_1
\]
这里，\(\sigma\) 是激活函数，\(w_1\) 是输入层到第一个隐藏层的权重，\(b_1\) 是偏置项。

对于第二层，我们有：
\[
z_2 = w_2 \cdot a_1 + b_2
\]
其中 \(w_2\) 是第一个隐藏层到第二层的权重。

### 2. 偏置 \(b_1\) 对 \(z_2\) 的影响

我们要研究的是偏置 \(b_1\) 的微小变化如何影响第二层的线性组合 \(z_2\)。利用链式法则，我们可以将 \(\Delta z_2\) 分解为以下形式：
\[
\Delta z_2 = \frac{\partial z_2}{\partial b_1} \Delta b_1
\]
为了求 \(\frac{\partial z_2}{\partial b_1}\)，我们可以进一步分解为：
\[
\frac{\partial z_2}{\partial b_1} = \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1}
\]

### 3. 分别求解各项

- \(\frac{\partial z_2}{\partial a_1} = w_2\)，因为 \(z_2 = w_2 \cdot a_1 + b_2\)。
- \(\frac{\partial a_1}{\partial z_1} = \sigma'(z_1)\)，这是激活函数的导数。
- \(\frac{\partial z_1}{\partial b_1} = 1\)，因为 \(z_1 = w_1 \cdot x + b_1\)，其中 \(b_1\) 的系数是1。

### 4. 代入各项求解

将这些结果代入，我们得到：
\[
\frac{\partial z_2}{\partial b_1} = w_2 \cdot \sigma'(z_1) \cdot 1 = w_2 \cdot \sigma'(z_1)
\]
因此，偏置 \(b_1\) 的微小变化 \(\Delta b_1\) 对 \(z_2\) 的影响为：
\[
\Delta z_2 = w_2 \cdot \sigma'(z_1) \cdot \Delta b_1
\]
这就是公式 \(\Delta z_2 \approx \sigma'(z_1) w_2 \Delta b_1\) 的推导过程。

### 5. 直观理解

这个公式的含义是：偏置 \(b_1\) 的变化首先影响 \(z_1\)，然后经过激活函数的导数 \(\sigma'(z_1)\) 调整，再乘以权重 \(w_2\)，最终影响到 \(z_2\)。
