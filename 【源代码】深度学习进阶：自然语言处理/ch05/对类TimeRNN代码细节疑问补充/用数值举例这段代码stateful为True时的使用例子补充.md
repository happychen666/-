好的，让我们通过一个具体的例子来说明当 `stateful=True` 时，RNN 如何在处理新的输入序列时保持隐藏状态，而不是重置为零。

### 示例代码

```python
import numpy as np

# 定义输入数据，形状为 (2, 3, 1)
inputs1 = np.array([
    [[1.0], [2.0], [3.0]],  # 第一个样本
    [[4.0], [5.0], [6.0]],  # 第二个样本
])  # 形状为 (2, 3, 1)

inputs2 = np.array([
    [[7.0], [8.0], [9.0]],  # 第一个样本
    [[10.0], [11.0], [12.0]],  # 第二个样本
])  # 形状为 (2, 3, 1)

# 定义权重和偏置
Wx = np.array([[0.5]])  # 输入到隐藏层的权重
Wh = np.array([[0.5]])  # 隐藏层到隐藏层的权重
b = np.array([[0.0]])    # 偏置

# 创建 TimeRNN 实例
rnn = TimeRNN(Wx, Wh, b, stateful=True)

# 进行前向传播，处理第一个输入序列
hs1 = rnn.forward(inputs1)
print("第一组输入序列的隐藏状态:\n", hs1)

# 再次进行前向传播，处理第二个输入序列
hs2 = rnn.forward(inputs2)
print("第二组输入序列的隐藏状态:\n", hs2)
```

### 执行步骤分析

1. **处理第一个输入序列 `inputs1`**:
   - **初始化**:
     - 因为 `stateful=True`，`self.h` 在第一次调用 `forward` 方法时被初始化为零，形状为 `(2, 1)`:
       \[
       self.h = [[0.0], [0.0]]
       \]

   - **时间步 0 (t=0)**:
     - 输入: `x = [[1.0], [4.0]]`
     - 计算:
       \[
       t = [[0.0], [0.0]] \cdot [[0.5]] + [[1.0], [4.0]] \cdot [[0.5]] + [[0.0]] = [[0.5], [2.0]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.462], [0.964]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.462], [0.964]]`

   - **时间步 1 (t=1)**:
     - 输入: `x = [[2.0], [5.0]]`
     - 计算:
       \[
       t = [[0.462], [0.964]] \cdot [[0.5]] + [[2.0], [5.0]] \cdot [[0.5]] + [[0.0]] = [[1.231], [2.982]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.845], [0.995]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.845], [0.995]]`

   - **时间步 2 (t=2)**:
     - 输入: `x = [[3.0], [6.0]]`
     - 计算:
       \[
       t = [[0.845], [0.995]] \cdot [[0.5]] + [[3.0], [6.0]] \cdot [[0.5]] + [[0.0]] = [[1.9225], [3.4975]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.958], [0.998]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.958], [0.998]]`

   - **输出**: `hs1` 包含 `inputs1` 的每个时间步的隐藏状态。

2. **处理第二个输入序列 `inputs2`**:
   - **继续使用上一个序列的隐藏状态**:
     - 在处理 `inputs2` 时，`self.h` 不会被重新初始化，而是继续使用上一个序列的状态，即 `self.h = [[0.958], [0.998]]`。

   - **时间步 0 (t=0)**:
     - 输入: `x = [[7.0], [10.0]]`
     - 计算:
       \[
       t = [[0.958], [0.998]] \cdot [[0.5]] + [[7.0], [10.0]] \cdot [[0.5]] + [[0.0]] = [[4.479], [5.499]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.999], [0.999]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.999], [0.999]]`

   - **时间步 1 (t=1)**:
     - 输入: `x = [[8.0], [11.0]]`
     - 计算:
       \[
       t = [[0.999], [0.999]] \cdot [[0.5]] + [[8.0], [11.0]] \cdot [[0.5]] + [[0.0]] = [[4.9995], [6.9995]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.999], [0.999]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.999], [0.999]]`

   - **时间步 2 (t=2)**:
     - 输入: `x = [[9.0], [12.0]]`
     - 计算:
       \[
       t = [[0.999], [0.999]] \cdot [[0.5]] + [[9.0], [12.0]] \cdot [[0.5]] + [[0.0]] = [[5.4995], [7.4995]]
       \]
     - 激活:
       \[
       h_{next} = \tanh(t) \approx [[0.999], [0.999]]
       \]
     - 更新 `self.h`:
       - `self.h = [[0.999], [0.999]]`

   - **输出**: `hs2` 包含 `inputs2` 的每个时间步的隐藏状态。

### 总结
- 在处理 `inputs1` 时，`self.h` 被初始化为零并随着序列的输入不断更新。
- 在处理 `inputs2` 时，`self.h` 保持为上一个序列的状态，这使得模型能够记忆并利用之前的上下文信息。
- 这种设计非常适合需要在时间序列中保持长期依赖关系的任务，比如自然语言处理和时间序列预测。