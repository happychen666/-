{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e6bccfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''转换为one-hot表示\n",
    "\n",
    "    :param corpus: 单词ID列表（一维或二维的NumPy数组）\n",
    "    :param vocab_size: 词汇个数\n",
    "    :return: one-hot表示（二维或三维的NumPy数组）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''生成上下文和目标词\n",
    "\n",
    "    :param corpus: 语料库（单词ID列表）\n",
    "    :param window_size: 窗口大小（当窗口大小为1时，左右各1个单词为上下文）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督标签\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        \n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    print(y,t)\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    # np.log(...)方法返回的是新数组\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "86ba2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        print('embedding forward',W,idx)\n",
    "        print('W[idx]==',W[idx])\n",
    "        \n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        print('self.grads=11111==',self.grads,self.idx)\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0 #dW[...] = 0 将 dW（即全零矩阵）重置为全零，确保每次进行反向传播时梯度是清零的。\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a872e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        print(V, H)\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        print('W_in===\\n',W_in)\n",
    "        print('W_out===\\n',W_out)\n",
    "        \n",
    "        # 生成层\n",
    "        self.in_layer0 = Embedding(W_in)\n",
    "        self.in_layer1 = Embedding(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        idx1 = contexts[:, 0].argmax(axis=1)\n",
    "        idx2 = contexts[:, 1].argmax(axis=1)\n",
    "        out = np.array([idx1,idx2])\n",
    "        print('out===', out)\n",
    "        h0 = self.in_layer0.forward(out[0, :])\n",
    "        h1 = self.in_layer1.forward(out[1, :])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        print('score===\\n',score)\n",
    "        print('target===\\n',target)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        print('loss',loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        print('da==',da)\n",
    "        in_layer1_da = self.in_layer1.backward(da)\n",
    "        in_layer2_da = self.in_layer0.backward(da)\n",
    "        print('in_layer1_da==',in_layer1_da)\n",
    "        print('in_layer2_da==',in_layer2_da)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a08a6ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts=== [[[1 0 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 1 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 0 0 1]]]\n",
      "5 3\n",
      "W_in===\n",
      " [[-0.00337228 -0.00564716 -0.0085281 ]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00895784  0.01889686 -0.00323562]\n",
      " [-0.01444123  0.00502983 -0.00387476]\n",
      " [ 0.01200857 -0.01226196  0.00648106]]\n",
      "W_out===\n",
      " [[ 0.00773472 -0.00883068 -0.00866825 -0.00940271  0.00857344]\n",
      " [-0.00166961  0.00868524 -0.01695678  0.00256694  0.00311968]\n",
      " [ 0.00635204 -0.001587   -0.01638275  0.02188129  0.00960893]]\n",
      "out=== [[0 1 1 1 2 1]\n",
      " [1 1 2 1 3 4]]\n",
      "embedding forward [[-0.00337228 -0.00564716 -0.0085281 ]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00895784  0.01889686 -0.00323562]\n",
      " [-0.01444123  0.00502983 -0.00387476]\n",
      " [ 0.01200857 -0.01226196  0.00648106]] [0 1 1 1 2 1]\n",
      "W[idx]== [[-0.00337228 -0.00564716 -0.0085281 ]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00895784  0.01889686 -0.00323562]\n",
      " [-0.00617044 -0.00270656  0.00763079]]\n",
      "embedding forward [[-0.00337228 -0.00564716 -0.0085281 ]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00895784  0.01889686 -0.00323562]\n",
      " [-0.01444123  0.00502983 -0.00387476]\n",
      " [ 0.01200857 -0.01226196  0.00648106]] [1 1 2 1 3 4]\n",
      "W[idx]== [[-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.00895784  0.01889686 -0.00323562]\n",
      " [-0.00617044 -0.00270656  0.00763079]\n",
      " [-0.01444123  0.00502983 -0.00387476]\n",
      " [ 0.01200857 -0.01226196  0.00648106]]\n",
      "score===\n",
      " [[-3.27812995e-05  6.56934299e-06  1.19535645e-04  2.43247850e-05\n",
      "  -5.82485336e-05]\n",
      " [ 5.26336953e-06  1.88719368e-05 -2.56318508e-05  2.18042740e-04\n",
      "   1.19782298e-05]\n",
      " [-5.80631058e-05  1.33617243e-04 -1.07702399e-04  1.39989148e-04\n",
      "  -1.84800101e-05]\n",
      " [ 5.26336953e-06  1.88719368e-05 -2.56318508e-05  2.18042740e-04\n",
      "   1.19782298e-05]\n",
      " [-1.33049500e-04  2.12861472e-04 -4.32016423e-05  6.29243514e-05\n",
      "  -9.71451009e-05]\n",
      " [ 7.98935143e-05 -1.01977712e-04 -1.39897111e-05  1.07733998e-04\n",
      "   6.94779010e-05]]\n",
      "target===\n",
      " [[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "[[0.19999106 0.19999893 0.20002152 0.20000248 0.19998595]\n",
      " [0.19999191 0.19999464 0.19998571 0.20003447 0.19999325]\n",
      " [0.19998482 0.20002314 0.19997488 0.20002444 0.19999273]\n",
      " [0.19999191 0.19999464 0.19998571 0.20003447 0.19999325]\n",
      " [0.19997329 0.20004247 0.19999124 0.20001246 0.19998047]\n",
      " [0.20001031 0.19997396 0.19999154 0.20001589 0.20000823]] [1 1 1 2 1 3]\n",
      "loss 1.6093867619832356\n",
      "da== [[ 0.0005593  -0.00079471  0.00046342]\n",
      " [ 0.00055931 -0.00079466  0.00046353]\n",
      " [ 0.0005593  -0.00079462  0.00046352]\n",
      " [ 0.00054577  0.00134218  0.00169651]\n",
      " [ 0.00055927 -0.00079463  0.00046346]\n",
      " [ 0.00060703 -0.00028482 -0.00149218]]\n",
      "self.grads=11111== [array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)] [1 1 2 1 3 4]\n",
      "self.grads=11111== [array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)] [0 1 1 1 2 1]\n",
      "in_layer1_da== [[ 0.          0.          0.        ]\n",
      " [ 0.00166438 -0.00024719  0.00262346]\n",
      " [ 0.0005593  -0.00079462  0.00046352]\n",
      " [ 0.00055927 -0.00079463  0.00046346]\n",
      " [ 0.00060703 -0.00028482 -0.00149218]]\n",
      "in_layer2_da== [[ 0.0005593  -0.00079471  0.00046342]\n",
      " [ 0.00227141 -0.00053192  0.00113139]\n",
      " [ 0.00055927 -0.00079463  0.00046346]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say say say I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "# print(corpus, word_to_id, id_to_word,'==corpus, word_to_id, id_to_word')\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "vocab_size = len(word_to_id)\n",
    "# print(contexts,target)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "print('contexts===',contexts)\n",
    "# idx1 = contexts[:, 0].argmax(axis=1)\n",
    "# idx2 = contexts[:, 1].argmax(axis=1)\n",
    "# print('idx===',idx2)\n",
    "hidden_size = 3\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "model.forward(contexts,target)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dd6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485f52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08d93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
