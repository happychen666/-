{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaaa78a-ce13-45b9-98b3-bee0bc02ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35258ac-e4cd-47f9-84fd-4c2db616a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b61f288-1f7b-4b5e-803f-500d18fafe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''转换为one-hot表示\n",
    "\n",
    "    :param corpus: 单词ID列表（一维或二维的NumPy数组）\n",
    "    :param vocab_size: 词汇个数\n",
    "    :return: one-hot表示（二维或三维的NumPy数组）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe1e34c-18c2-4986-a4e0-c755ab9f26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''生成上下文和目标词\n",
    "\n",
    "    :param corpus: 语料库（单词ID列表）\n",
    "    :param window_size: 窗口大小（当窗口大小为1时，左右各1个单词为上下文）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f2d2ae5-12fb-4cbc-8429-975eaae58a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督标签\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        \n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e9fa8a-232e-4e21-b8a0-0c92dd551be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2850e17-75c3-4bef-8248-f93cc2eaaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    # np.log(...)方法返回的是新数组\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caad8836-8cf2-4f73-b186-d4727fc7659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        print(V, H)\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        print('W_in===\\n',W_in)\n",
    "        print('W_out===\\n',W_out)\n",
    "        \n",
    "        # 生成层\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        print('contexts[:, 0]== \\n',contexts[:, 0])\n",
    "        print('contexts[:, 1]== \\n',contexts[:, 1])\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        print('h0== \\n',h0)\n",
    "        print('h1== \\n',h1)\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        print('score===\\n',score)\n",
    "        print('target===\\n',target)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        print('loss',loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8674fb2-64b0-4092-ae13-c57b008156f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 2 1 3 4] {'you': 0, 'say': 1, 'i': 2, 'hello': 3, '.': 4} {0: 'you', 1: 'say', 2: 'i', 3: 'hello', 4: '.'} ==corpus, word_to_id, id_to_word\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [2 3]\n",
      " [1 4]] [1 1 1 2 1 3]\n",
      "[[[1 0 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 1 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 0 0 1]]] 5\n",
      "5 5\n",
      "W_in===\n",
      " [[ 0.02563999 -0.00311403  0.01157996  0.00589074 -0.00337122]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [ 0.02793715  0.00494063 -0.01464814  0.00151682  0.00098777]\n",
      " [-0.02579672 -0.01234641 -0.00766119  0.00301793  0.0002799 ]\n",
      " [-0.00290042  0.01070336 -0.00263139 -0.00228988 -0.01353024]]\n",
      "W_out===\n",
      " [[ 9.3219038e-03  1.3446729e-02  1.2954857e-02 -7.6858094e-03\n",
      "   7.1504933e-04]\n",
      " [-1.7743864e-03 -1.6152894e-03 -1.5440615e-02 -4.4043818e-03\n",
      "   4.4626016e-03]\n",
      " [-2.2788631e-02 -7.2356304e-03 -1.2632892e-02  7.2004659e-05\n",
      "  -1.9538665e-02]\n",
      " [ 1.2394680e-02 -4.4498979e-03  3.8386597e-03  2.6493728e-02\n",
      "   1.3055851e-02]\n",
      " [-1.0535759e-02  9.5258476e-03  9.8903449e-03 -1.9914966e-02\n",
      "  -8.1964293e-03]]\n",
      "contexts[:, 0]== \n",
      " [[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]]\n",
      "contexts[:, 1]== \n",
      " [[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "h0== \n",
      " [[ 0.02563999 -0.00311403  0.01157996  0.00589074 -0.00337122]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [ 0.02793715  0.00494063 -0.01464814  0.00151682  0.00098777]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]]\n",
      "h1== \n",
      " [[-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [ 0.02793715  0.00494063 -0.01464814  0.00151682  0.00098777]\n",
      " [-0.00244224 -0.00047083  0.0110317  -0.0089099   0.00374297]\n",
      " [-0.02579672 -0.01234641 -0.00766119  0.00301793  0.0002799 ]\n",
      " [-0.00290042  0.01070336 -0.00263139 -0.00228988 -0.01353024]]\n",
      "score===\n",
      " [[-1.67009427e-04  8.55454782e-05  3.11562309e-05 -1.24134213e-04\n",
      "  -2.41838302e-04]\n",
      " [-4.23198644e-04 -3.65977679e-05 -1.60914007e-04 -2.88959112e-04\n",
      "  -3.66397464e-04]\n",
      " [ 8.53334067e-05  2.19866582e-04  1.62680897e-04 -2.52989520e-04\n",
      "  -1.32303528e-05]\n",
      " [-4.23198644e-04 -3.65977679e-05 -1.60914007e-04 -2.88959112e-04\n",
      "  -3.66397464e-04]\n",
      " [ 2.92171714e-04  9.70314039e-05  2.26927634e-04  5.47286914e-05\n",
      "   2.26595245e-04]\n",
      " [-1.47546669e-04 -9.62727380e-05 -2.36560934e-04 -5.26057693e-05\n",
      "  -9.41447517e-05]]\n",
      "target===\n",
      " [[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "loss 1.6093288183535606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6093288183535606"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You say say say I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus, word_to_id, id_to_word,'==corpus, word_to_id, id_to_word')\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "vocab_size = len(word_to_id)\n",
    "print(contexts,target)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "print(contexts,vocab_size)\n",
    "\n",
    "hidden_size = 5\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "model.forward(contexts,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b628e3-868d-4b1b-832d-c80f7c1153bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578cb7b-4ece-467d-8fe1-d31a499aa6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe0cd8-2090-473a-beb6-a901f9394a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
