{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e6bccfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''转换为one-hot表示\n",
    "\n",
    "    :param corpus: 单词ID列表（一维或二维的NumPy数组）\n",
    "    :param vocab_size: 词汇个数\n",
    "    :return: one-hot表示（二维或三维的NumPy数组）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''生成上下文和目标词\n",
    "\n",
    "    :param corpus: 语料库（单词ID列表）\n",
    "    :param window_size: 窗口大小（当窗口大小为1时，左右各1个单词为上下文）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督标签\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        \n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    print(y,t)\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    # np.log(...)方法返回的是新数组\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86ba2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        print('embedding forward',W,idx)\n",
    "        print('W[idx]==',W[idx])\n",
    "        \n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        print('self.grads=11111==',self.grads)\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0 #dW[...] = 0 将 dW（即全零矩阵）重置为全零，确保每次进行反向传播时梯度是清零的。\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a872e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        print(V, H)\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "        print('W_in===\\n',W_in)\n",
    "        print('W_out===\\n',W_out)\n",
    "        \n",
    "        # 生成层\n",
    "        self.in_layer0 = Embedding(W_in)\n",
    "        self.in_layer1 = Embedding(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        idx1 = contexts[:, 0].argmax(axis=1)\n",
    "        idx2 = contexts[:, 1].argmax(axis=1)\n",
    "        out = np.array([idx1,idx2])\n",
    "        print('out===', out)\n",
    "        h0 = self.in_layer0.forward(out[0, :])\n",
    "        h1 = self.in_layer1.forward(out[1, :])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        print('score===\\n',score)\n",
    "        print('target===\\n',target)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        print('loss',loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        print('da==',da)\n",
    "        in_layer1_da = self.in_layer1.backward(da)\n",
    "        in_layer2_da = self.in_layer0.backward(da)\n",
    "        print('in_layer1_da==',in_layer1_da)\n",
    "        print('in_layer2_da==',in_layer2_da)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a08a6ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts=== [[[1 0 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 1 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 0 0 1]]]\n",
      "5 3\n",
      "W_in===\n",
      " [[-0.00680847 -0.01314693  0.01695464]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.01113281  0.00531954  0.02020944]\n",
      " [-0.00267802 -0.01366908 -0.00189135]\n",
      " [ 0.00215178  0.01119256  0.00898341]]\n",
      "W_out===\n",
      " [[-0.0048501  -0.00122238 -0.00635452 -0.00325648  0.00097178]\n",
      " [ 0.00184159  0.00836979  0.01082706  0.00772464 -0.01042586]\n",
      " [ 0.00659045  0.00580951  0.00637852  0.00141428 -0.00024498]]\n",
      "out=== [[0 1 1 1 2 1]\n",
      " [1 1 2 1 3 4]]\n",
      "embedding forward [[-0.00680847 -0.01314693  0.01695464]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.01113281  0.00531954  0.02020944]\n",
      " [-0.00267802 -0.01366908 -0.00189135]\n",
      " [ 0.00215178  0.01119256  0.00898341]] [0 1 1 1 2 1]\n",
      "W[idx]== [[-0.00680847 -0.01314693  0.01695464]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.01113281  0.00531954  0.02020944]\n",
      " [-0.00084524  0.00029925  0.0077658 ]]\n",
      "embedding forward [[-0.00680847 -0.01314693  0.01695464]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.01113281  0.00531954  0.02020944]\n",
      " [-0.00267802 -0.01366908 -0.00189135]\n",
      " [ 0.00215178  0.01119256  0.00898341]] [1 1 2 1 3 4]\n",
      "W[idx]== [[-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.01113281  0.00531954  0.02020944]\n",
      " [-0.00084524  0.00029925  0.0077658 ]\n",
      " [-0.00267802 -0.01366908 -0.00189135]\n",
      " [ 0.00215178  0.01119256  0.00898341]]\n",
      "score===\n",
      " [[ 8.81899323e-05  2.27185701e-05  3.36064259e-05 -1.96789933e-05\n",
      "   6.02272376e-05]\n",
      " [ 5.58307147e-05  4.86534009e-05  5.81454042e-05  1.60471209e-05\n",
      "  -5.84378540e-06]\n",
      " [ 1.26405765e-04  1.12096153e-04  1.57695104e-04  6.09870949e-05\n",
      "  -3.85370076e-05]\n",
      " [ 5.58307147e-05  4.86534009e-05  5.81454042e-05  1.60471209e-05\n",
      "  -5.84378540e-06]\n",
      " [ 8.61658918e-05  2.67086871e-05  5.71012097e-05  3.19216997e-06\n",
      "   3.45713052e-05]\n",
      " [ 6.26055698e-05  9.59458412e-05  1.11477595e-04  5.41017362e-05\n",
      "  -6.13228040e-05]]\n",
      "target===\n",
      " [[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "[[0.20001023 0.19999713 0.19999929 0.19998866 0.20000464]\n",
      " [0.20000428 0.20000282 0.20000473 0.19999631 0.19999194]\n",
      " [0.20000853 0.20000567 0.20001478 0.19999544 0.19997555]\n",
      " [0.20000428 0.20000282 0.20000473 0.19999631 0.19999194]\n",
      " [0.20000893 0.19999702 0.2000031  0.19999233 0.1999986 ]\n",
      " [0.20000203 0.20000868 0.20001179 0.2000003  0.19997722]] [1 1 1 2 1 3]\n",
      "loss 1.6094309488932292\n",
      "da== [[-1.4332989e-04 -3.9187446e-04 -1.5166067e-04]\n",
      " [-1.4333405e-04 -3.9185051e-04 -1.5165711e-04]\n",
      " [-1.4334249e-04 -3.9182516e-04 -1.5164781e-04]\n",
      " [ 2.8434413e-04 -5.9662311e-04 -1.9907413e-04]\n",
      " [-1.4333286e-04 -3.9186372e-04 -1.5165884e-04]\n",
      " [ 2.6168502e-05 -3.3806305e-04  2.1461844e-04]]\n",
      "self.grads=11111== [[-1.4332989e-04 -3.9187446e-04 -1.5166067e-04]\n",
      " [-1.4333405e-04 -3.9185051e-04 -1.5165711e-04]\n",
      " [-1.4334249e-04 -3.9182516e-04 -1.5164781e-04]\n",
      " [ 2.8434413e-04 -5.9662311e-04 -1.9907413e-04]\n",
      " [-1.4333286e-04 -3.9186372e-04 -1.5165884e-04]\n",
      " [ 2.6168502e-05 -3.3806305e-04  2.1461844e-04]]\n",
      "self.grads=11111== [[-1.4332989e-04 -3.9187446e-04 -1.5166067e-04]\n",
      " [-1.4333405e-04 -3.9185051e-04 -1.5165711e-04]\n",
      " [-1.4334249e-04 -3.9182516e-04 -1.5164781e-04]\n",
      " [ 2.8434413e-04 -5.9662311e-04 -1.9907413e-04]\n",
      " [-1.4333286e-04 -3.9186372e-04 -1.5165884e-04]\n",
      " [ 2.6168502e-05 -3.3806305e-04  2.1461844e-04]]\n",
      "in_layer1_da== [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-2.3198081e-06 -1.3803481e-03 -5.0239195e-04]\n",
      " [-1.4334249e-04 -3.9182516e-04 -1.5164781e-04]\n",
      " [-1.4333286e-04 -3.9186372e-04 -1.5165884e-04]\n",
      " [ 2.6168502e-05 -3.3806305e-04  2.1461844e-04]]\n",
      "in_layer2_da== [[-1.4332989e-04 -3.9187446e-04 -1.5166067e-04]\n",
      " [ 2.3836092e-05 -1.7183619e-03 -2.8776057e-04]\n",
      " [-1.4333286e-04 -3.9186372e-04 -1.5165884e-04]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say say say I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "# print(corpus, word_to_id, id_to_word,'==corpus, word_to_id, id_to_word')\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "vocab_size = len(word_to_id)\n",
    "# print(contexts,target)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "print('contexts===',contexts)\n",
    "# idx1 = contexts[:, 0].argmax(axis=1)\n",
    "# idx2 = contexts[:, 1].argmax(axis=1)\n",
    "# print('idx===',idx2)\n",
    "hidden_size = 3\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "model.forward(contexts,target)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dd6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485f52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08d93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
