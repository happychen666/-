理解为什么 \(\frac{\partial C}{\partial a_j^L} = a_j^L - y_j\) 和 \(\sigma'(z_j^L) = a_j^L \cdot (1 - a_j^L)\) 的关系需要区分交叉熵损失函数对输出激活值 \(a_j^L\) 的偏导数和对带权输入 \(z_j^L\) 的偏导数。

### 交叉熵损失函数

交叉熵损失函数 \(C\) 定义为：

\[ C = - \sum_{j} \left[ y_j \log(a_j^L) + (1 - y_j) \log(1 - a_j^L) \right] \]

### 损失函数对激活值的偏导数

我们首先计算损失函数 \(C\) 对输出层激活值 \(a_j^L\) 的偏导数：

\[ \frac{\partial C}{\partial a_j^L} = - \left[ \frac{y_j}{a_j^L} - \frac{1 - y_j}{1 - a_j^L} \right] \]

进一步化简：

\[ \frac{\partial C}{\partial a_j^L} = - \frac{y_j}{a_j^L} + \frac{1 - y_j}{1 - a_j^L} \]

再化简：

\[ \frac{\partial C}{\partial a_j^L} = \frac{(1 - y_j) \cdot a_j^L - y_j \cdot (1 - a_j^L)}{a_j^L \cdot (1 - a_j^L)} \]

可以进一步得出：

\[ \frac{\partial C}{\partial a_j^L} = \frac{a_j^L - y_j}{a_j^L \cdot (1 - a_j^L)} \]

### 损失函数对带权输入的偏导数

在神经网络中，输出层的激活值 \(a_j^L\) 是通过 sigmoid 激活函数计算得到的：

\[ a_j^L = \sigma(z_j^L) = \frac{1}{1 + e^{-z_j^L}} \]

sigmoid 函数的导数是：

\[ \sigma'(z_j^L) = a_j^L \cdot (1 - a_j^L) \]

我们还需要计算损失函数 \(C\) 对带权输入 \(z_j^L\) 的偏导数，这样我们就可以用链式法则将两者联系起来。

\[ \frac{\partial C}{\partial z_j^L} = \frac{\partial C}{\partial a_j^L} \cdot \frac{\partial a_j^L}{\partial z_j^L} \]

我们已经知道：

\[ \frac{\partial a_j^L}{\partial z_j^L} = \sigma'(z_j^L) = a_j^L \cdot (1 - a_j^L) \]

把之前的结果代入：

\[ \frac{\partial C}{\partial z_j^L} = \frac{\partial C}{\partial a_j^L} \cdot a_j^L \cdot (1 - a_j^L) \]

结合之前的推导：

\[ \frac{\partial C}{\partial a_j^L} = \frac{a_j^L - y_j}{a_j^L \cdot (1 - a_j^L)} \]

所以：

\[ \frac{\partial C}{\partial z_j^L} = \left( \frac{a_j^L - y_j}{a_j^L \cdot (1 - a_j^L)} \right) \cdot a_j^L \cdot (1 - a_j^L) \]

这两个项完全抵消：

\[ \frac{\partial C}{\partial z_j^L} = a_j^L - y_j \]

### 结论

因此，对于交叉熵损失函数，一个训练样本 \(x\) 的输出误差 \(\delta^L\) 为：

\[ \delta_j^L = \frac{\partial C}{\partial z_j^L} = a_j^L - y_j \]

这个结果表明，输出误差等于预测值与实际标签之间的差异。

总结来说，\(\frac{\partial C}{\partial a_j^L} = a_j^L - y_j\) 的推导是通过交叉熵损失函数的性质直接得到的，而 \(\sigma'(z_j^L) = a_j^L \cdot (1 - a_j^L)\) 则是 sigmoid 函数的导数。它们之间的关系通过链式法则在计算损失函数对带权输入的偏导数时体现出来，最终得到 \(\delta_j^L = a_j^L - y_j\)。
